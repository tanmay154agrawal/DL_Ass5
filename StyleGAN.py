# -*- coding: utf-8 -*-
"""m22ma011_qu2_assignment5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aeVCIXtOzaV2EbVaw-5WbHwvV9GNbVVf
"""

!git clone https://github.com/NVlabs/stylegan3.git

cd stylegan3

!pip install ninja

for i in range(10):
 !python gen_images.py --outdir=out --trunc=1 --seeds={i} \
    --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl

loc1=['/content/stylegan3/out/seed0001.png','/content/stylegan3/out/seed0002.png','/content/stylegan3/out/seed0003.png','/content/stylegan3/out/seed0004.png','/content/stylegan3/out/seed0005.png','/content/stylegan3/out/seed0006.png','/content/stylegan3/out/seed0007.png','/content/stylegan3/out/seed0008.png','/content/stylegan3/out/seed0009.png','/content/stylegan3/out/seed0000.png']

import cv2
from google.colab.patches import cv2_imshow

for loc in loc1: 
  img=cv2.imread(loc)
  cv2_imshow(img)

cd /content

!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git

cd /content/stylegan2-ada-pytorch

NETWORK = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
STEPS = 150
FPS = 30
FREEZE_STEPS = 30

import sys
sys.path.insert(0, "/content/stylegan2-ada-pytorch")

!wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2
!bzip2 -d shape_predictor_5_face_landmarks.dat.bz2

!pip install ninja

import os
from google.colab import files
import cv2
import numpy as np
from PIL import Image
import dlib
from matplotlib import pyplot as plt
import cv2
import torch
import dnnlib
import legacy
import PIL.Image
import numpy as np
import imageio
from tqdm.notebook import tqdm
from IPython.display import HTML
from base64 import b64encode

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')

def find_eyes(img):
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  rects = detector(gray, 0)
  
  if len(rects) == 0:
    raise ValueError("No faces detected")
  elif len(rects) > 1:
    raise ValueError("Multiple faces detected")

  shape = predictor(gray, rects[0])
  features = []

  for i in range(0, 5):
    features.append((i, (shape.part(i).x, shape.part(i).y)))

  return (int(features[3][1][0] + features[2][1][0]) // 2, \
    int(features[3][1][1] + features[2][1][1]) // 2), \
    (int(features[1][1][0] + features[0][1][0]) // 2, \
    int(features[1][1][1] + features[0][1][1]) // 2)

def crop_stylegan(img):
  left_eye, right_eye = find_eyes(img)
  d = abs(right_eye[0] - left_eye[0])
  z = 255/d
  ar = img.shape[0]/img.shape[1]
  w = img.shape[1] * z
  img2 = cv2.resize(img, (int(w), int(w*ar)))
  bordersize = 1024
  img3 = cv2.copyMakeBorder(
      img2,
      top=bordersize,
      bottom=bordersize,
      left=bordersize,
      right=bordersize,
      borderType=cv2.BORDER_REPLICATE)

  left_eye2, right_eye2 = find_eyes(img3)

  crop1 = left_eye2[0] - 385 
  crop0 = left_eye2[1] - 490
  return img3[crop0:crop0+1024,crop1:crop1+1024]

def source(): 
  uploaded = files.upload()
  for k, v in uploaded.items():
    _, ext = os.path.splitext(k)
    os.remove(k)
    SOURCE_NAME = f"source{ext}"
    open(SOURCE_NAME, 'wb').write(v)
  return cv2.imread(SOURCE_NAME)
def target():
  uploaded = files.upload()
  for k, v in uploaded.items():
    _, ext = os.path.splitext(k)
    os.remove(k)
    TARGET_NAME = f"target{ext}"
    open(TARGET_NAME, 'wb').write(v)
  return cv2.imread(TARGET_NAME)
def interpolation(image_source,image_target):
  cropped_source = crop_stylegan(image_source)
  cropped_target = crop_stylegan(image_target)
  img = cv2.cvtColor(cropped_source, cv2.COLOR_BGR2RGB)
  plt.imshow(img)
  plt.title('source')
  plt.show()

  img = cv2.cvtColor(cropped_target, cv2.COLOR_BGR2RGB)
  plt.imshow(img)
  plt.title('target')
  plt.show()
  cv2.imwrite("cropped_source.png", cropped_source)
  cv2.imwrite("cropped_target.png", cropped_target)

  !python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 100 --outdir=out_source --target=cropped_source.png --network={NETWORK}
  !python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 100 --outdir=out_target --target=cropped_target.png --network={NETWORK}
  lvec2 = np.load('/content/stylegan2-ada-pytorch/out_source/projected_w.npz')['w']
  lvec1 = np.load('/content/stylegan2-ada-pytorch/out_target/projected_w.npz')['w']

  network_pkl = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
  device = torch.device('cuda')
  with dnnlib.util.open_url(network_pkl) as fp:
      G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore
  diff = lvec2 - lvec1
  step = diff / STEPS
  current = lvec1.copy()
  target_uint8 = np.array([1024,1024,3], dtype=np.uint8)

  video = imageio.get_writer('/content/movie.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')
  for j in tqdm(range(STEPS)):
    z = torch.from_numpy(current).to(device)
    synth_image = G.synthesis(z, noise_mode='const')
    synth_image = (synth_image + 1) * (255/2)
    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()

    repeat = FREEZE_STEPS if j==0 or j==(STEPS-1) else 1
    
    for i in range(repeat):
      video.append_data(synth_image)
    current = current + step


  video.close()
  mp4 = open('/content/movie.mp4','rb').read()
  data_url = "data:/content/movie.mp4;base64," + b64encode(mp4).decode()
  HTML("""
  <video width=400 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url)

def frames(): 
  filename = "/content/movie.mp4"
  video = cv2.VideoCapture(filename)
  time_points = [0, 1, 2, 3, 4,5,6]
  for i, time_point in enumerate(time_points):
      # Set the video's current position to the desired time point
      video.set(cv2.CAP_PROP_POS_MSEC, time_point * 1000)

      # Read the next frame of the video
      ret, frame = video.read()

      # Check if there are no more frames
      if not ret:
          break

      # Save the current frame as an image file
      filename = f"frame{i}.jpg"
      cv2.imwrite(filename, frame)

  # Release the video file
  video.release()
  fig, axs = plt.subplots(1, 6,figsize=(20, 12))
  plt.subplots_adjust(wspace=0, hspace=0)
  import os
  directory="/content/frames"
  files = os.listdir(directory)
  for i, filename in enumerate(files):
      f=os.path.join(directory, filename)
      img=cv2.imread(f)
      if i==6:
        break
      axs[i].imshow(img)
      axs[i].set_xticks([])
      axs[i].set_yticks([])
  plt.show()

src=source()
tgt=target()
interpolation(src,tgt)

cd /content/frames

frames()

